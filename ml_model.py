# -*- coding: utf-8 -*-
"""ML-MODEL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nQ_ub94XtiTYY4Vntwi6Hdr5V4LzMwT2
"""



# ============================================================================
# SECTION 1: ENVIRONMENT SETUP & INSTALLATION
# ============================================================================

!pip install -q xgboost lightgbm shap optuna scikit-learn pandas numpy matplotlib seaborn imbalanced-learn pdfkit wkhtmltopdf
!pip install --upgrade xgboost

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.metrics import (classification_report, confusion_matrix,
                             accuracy_score, roc_auc_score, roc_curve, auc)
from sklearn.calibration import CalibratedClassifierCV
import xgboost as xgb
import lightgbm as lgb
import shap
import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances
import warnings
warnings.filterwarnings('ignore')

# Set style
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

print("✅ All libraries imported successfully!")

# ============================================================================
# SECTION 2: DATA LOADING & EXPLORATION
# ============================================================================

# Load dataset
DATA_PATH = '/content/kepler_koi.csv'  # Update if using different path

# Alternative: Download directly from NASA
import urllib.request
url = 'https://exoplanetarchive.ipac.caltech.edu/cgi-bin/nstedAPI/nph-nstedAPI?table=cumulative'
try:
    df = pd.read_csv(DATA_PATH)
    print(f"✅ Dataset loaded from local path: {DATA_PATH}")
except FileNotFoundError:
    print("📥 Downloading dataset from NASA Exoplanet Archive...")
    df = pd.read_csv(url)
    df.to_csv(DATA_PATH, index=False)
    print("✅ Dataset downloaded and saved!")

print(f"\\n📊 Dataset Shape: {df.shape}")
print(f"\\n🔍 First Few Rows:")
print(df.head())
print(f"\\n📋 Column Names:")
print(df.columns.tolist())
print(f"\\n🎯 Target Distribution:")
print(df['koi_pdisposition'].value_counts())

# ============================================================================
# SECTION 3: DATA PREPROCESSING
# ============================================================================

print("\\n" + "="*80)
print("STAGE 1: DATA PREPROCESSING")
print("="*80)

# Select relevant features
feature_columns = [
    'koi_period', 'koi_duration', 'koi_depth', 'koi_prad', 'koi_teq',
    'koi_insol', 'koi_smass', 'koi_srad', 'koi_steff', 'koi_srho',
    'ra', 'dec', 'koi_kepmag', 'koi_time0bk', 'koi_impact',
    'koi_duration', 'koi_ingress', 'koi_model_snr', 'koi_tce_plnt_num',
    'koi_steff_err1', 'koi_slogg', 'koi_smet'
]

# Keep only existing columns
feature_columns = [col for col in feature_columns if col in df.columns]

# Remove 'koi_ingress' as it has all missing values
if 'koi_ingress' in feature_columns:
    feature_columns.remove('koi_ingress')

# Prepare target variable
target_col = 'koi_pdisposition' if 'koi_pdisposition' in df.columns else 'koi_disposition'
df_clean = df[[target_col] + feature_columns].copy()

# Remove rows with missing target
df_clean = df_clean.dropna(subset=[target_col])

# Encode target variable
le = LabelEncoder()
df_clean['target'] = le.fit_transform(df_clean[target_col])
class_names = le.classes_
print(f"\\n🎯 Classes: {class_names}")
print(f"Target Encoding: {dict(zip(class_names, le.transform(class_names)))}")

# Separate features and target
X = df_clean[feature_columns]
y = df_clean['target']

print(f"\\n📊 Features shape: {X.shape}")
print(f"Missing values per column:")
print(X.isnull().sum()[X.isnull().sum() > 0])

# Impute missing values
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

print(f"\\n✅ Missing values imputed!")

# ============================================================================
# SECTION 4: FEATURE ENGINEERING
# ============================================================================

print("\\n" + "="*80)
print("STAGE 2: ADVANCED FEATURE ENGINEERING")
print("="*80)

X_engineered = X_imputed.copy()

# 1. Stellar Luminosity (Stefan-Boltzmann approximation)
if 'koi_srad' in X_engineered.columns and 'koi_steff' in X_engineered.columns:
    X_engineered['stellar_luminosity'] = (X_engineered['koi_srad']**2) * (X_engineered['koi_steff']**4)
    X_engineered['stellar_luminosity'] = np.log1p(X_engineered['stellar_luminosity'])
    print("✅ Created: stellar_luminosity")

# 2. Relative planet size
if 'koi_prad' in X_engineered.columns and 'koi_srad' in X_engineered.columns:
    X_engineered['relative_size'] = X_engineered['koi_prad'] / (X_engineered['koi_srad'] + 1e-6)
    print("✅ Created: relative_size")

# 3. Temperature ratio
if 'koi_teq' in X_engineered.columns and 'koi_steff' in X_engineered.columns:
    X_engineered['temp_ratio'] = X_engineered['koi_teq'] / (X_engineered['koi_steff'] + 1e-6)
    print("✅ Created: temp_ratio")

# 4. Stellar flux ratio
if 'koi_insol' in X_engineered.columns and 'koi_srho' in X_engineered.columns:
    X_engineered['stellar_flux_ratio'] = X_engineered['koi_insol'] / (X_engineered['koi_srho'] + 1e-6)
    print("✅ Created: stellar_flux_ratio")

# 5. Orbital energy proxy
if 'koi_smass' in X_engineered.columns and 'koi_period' in X_engineered.columns:
    # Semi-major axis approximation using Kepler's third law
    X_engineered['orbital_energy'] = X_engineered['koi_smass'] / (X_engineered['koi_period']**(2/3) + 1e-6)
    print("✅ Created: orbital_energy")

# 6. Habitability Index (composite score)
if all(col in X_engineered.columns for col in ['koi_teq', 'koi_prad', 'koi_insol']):
    # Normalize each component
    teq_norm = (X_engineered['koi_teq'] - 200) / 150  # Centered around habitable zone
    prad_norm = (X_engineered['koi_prad'] - 1) / 2    # Earth-sized = 1
    insol_norm = (X_engineered['koi_insol'] - 1) / 2  # Earth insolation = 1

    X_engineered['habitability_index'] = np.exp(-(teq_norm**2 + prad_norm**2 + insol_norm**2))
    print("✅ Created: habitability_index")

# 7. Equilibrium zone flag
if 'koi_teq' in X_engineered.columns:
    X_engineered['in_habitable_zone'] = ((X_engineered['koi_teq'] > 200) &
                                          (X_engineered['koi_teq'] < 350)).astype(int)
    print("✅ Created: in_habitable_zone")

# 8. Planet density estimate
if 'koi_prad' in X_engineered.columns:
    X_engineered['planet_density_proxy'] = 1 / (X_engineered['koi_prad']**3 + 1e-6)
    print("✅ Created: planet_density_proxy")

print(f"\\n📊 Engineered features shape: {X_engineered.shape}")

# Replace inf and -inf with NaN, then fill with median
X_engineered = X_engineered.replace([np.inf, -np.inf], np.nan)
X_engineered = X_engineered.fillna(X_engineered.median())

# ============================================================================
# SECTION 5: FEATURE SCALING & SPLIT
# ============================================================================

# Apply RobustScaler (resistant to outliers)
scaler = RobustScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_engineered),
                        columns=X_engineered.columns)

# Train-test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\\n📊 Train set: {X_train.shape}, Test set: {X_test.shape}")
print(f"Train distribution:\\n{y_train.value_counts()}")
print(f"Test distribution:\\n{y_test.value_counts()}")

# ============================================================================
# SECTION 6: CORRELATION ANALYSIS
# ============================================================================

print("\\n" + "="*80)
print("STAGE 3: FEATURE CORRELATION ANALYSIS")
print("="*80)

plt.figure(figsize=(16, 12))
correlation_matrix = X_engineered.corr()
sns.heatmap(correlation_matrix, cmap='coolwarm', center=0,
            square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')
plt.show()

# ============================================================================
# SECTION 7: HYPERPARAMETER OPTIMIZATION WITH OPTUNA
# ============================================================================

print("\n" + "="*80)
print("STAGE 4: HYPERPARAMETER OPTIMIZATION")
print("="*80)

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
        'max_depth': trial.suggest_int('max_depth', 6, 12),
        'subsample': trial.suggest_float('subsample', 0.7, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 5.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 2.0),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),
        'gamma': trial.suggest_float('gamma', 0, 0.5),
        'random_state': 42,
        'n_jobs': -1,
        'eval_metric': 'mlogloss'
    }

    model = xgb.XGBClassifier(**params)

    # Stratified K-Fold CV
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    # === FIX: Convert to NumPy arrays to resolve the XGBoost/Pandas version conflict ===
    X_train_np = X_train.values
    y_train_np = y_train.values

    scores = cross_val_score(model, X_train_np, y_train_np, cv=skf,
                             scoring='accuracy', n_jobs=-1)

    return scores.mean()

print("🔍 Running Optuna hyperparameter optimization (this may take 5-10 minutes)...")
study = optuna.create_study(direction='maximize', study_name='xgboost_optimization')
study.optimize(objective, n_trials=50, show_progress_bar=True)

print(f"\n✅ Best trial accuracy: {study.best_trial.value:.4f}")
print(f"Best hyperparameters:")
for key, value in study.best_params.items():
    print(f"  {key}: {value}")

!pip install kaleido

# ============================================================================
# SECTION 5: FEATURE SCALING & SPLIT (Correction/Verification)
# ============================================================================

# ... (Existing split code)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# === NEW VERIFICATION CHECK ===
num_classes = y_train.nunique()

print(f"\n📊 Unique classes in y_train: {num_classes}")
print(f"Train distribution:\n{y_train.value_counts()}")

if num_classes < 2:
    raise ValueError(f"Target variable has only {num_classes} class(es). Cannot run multi-class classification.")
# ===============================

# ============================================================================
# SECTION 8: FINAL MODEL TRAINING
# ============================================================================

print("\n" + "="*80)
print("STAGE 5: FINAL MODEL TRAINING")
print("="*80)

# Use best parameters from Optuna
best_params = study.best_params.copy()

# Since the data has 2 classes (0 and 1), it is a binary classification problem.
best_params.update({
    'random_state': 42,
    'n_jobs': -1,
    'eval_metric': 'logloss'  # <-- UPDATED: Use binary logloss instead of mlogloss
    # The 'num_class' parameter is omitted to allow XGBoost to correctly infer
    # the binary:logistic objective from the 2-class data.
})

# Train final XGBoost model
final_model = xgb.XGBClassifier(**best_params)

# FIX APPLIED: Convert all data inputs to NumPy arrays using .values
final_model.fit(
    X_train.values, y_train.values,  # X_train and y_train converted
    # All components in eval_set must also be converted
    eval_set=[(X_train.values, y_train.values), (X_test.values, y_test.values)],
    verbose=False
)

# Predictions
# FIX APPLIED: X_test converted for prediction methods
y_pred = final_model.predict(X_test.values)
y_pred_proba = final_model.predict_proba(X_test.values)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\n🎯 Final Model Accuracy: {accuracy*100:.2f}%")

if accuracy >= 0.95:
    print("✅ TARGET ACHIEVED: Accuracy >= 95%!")
else:
    print(f"⚠️ Target not met. Current: {accuracy*100:.2f}%, Target: 95.00%")

# ============================================================================
# SECTION 9: MODEL EVALUATION
# ============================================================================

print("\\n" + "="*80)
print("STAGE 6: COMPREHENSIVE MODEL EVALUATION")
print("="*80)

# Classification Report
print("\\n📊 Classification Report:")
print(classification_report(y_test, y_pred, target_names=class_names))

# Confusion Matrix
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

# ROC-AUC Curves for each class
plt.figure(figsize=(12, 8))
for i, class_name in enumerate(class_names):
    fpr, tpr, _ = roc_curve(y_test == i, y_pred_proba[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{class_name} (AUC = {roc_auc:.3f})', linewidth=2)

plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curves for Multi-Class Classification', fontsize=16, fontweight='bold')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')
plt.show()

# Feature Importance
plt.figure(figsize=(12, 10))
feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': final_model.feature_importances_
}).sort_values('importance', ascending=False).head(20)

sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')
plt.title('Top 20 Feature Importances', fontsize=16, fontweight='bold')
plt.xlabel('Importance Score')
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

# ============================================================================
# SECTION 10: SHAP EXPLAINABILITY
# ============================================================================

print("\n" + "="*80)
print("STAGE 7: SHAP EXPLAINABILITY ANALYSIS")
print("="*80)

# Create SHAP explainer
print("🔍 Computing SHAP values (this may take a few minutes)...")
explainer = shap.TreeExplainer(final_model)

# === Calculate SHAP values using the NumPy array (Required fix for AttributeError) ===
X_test_np = X_test.values
shap_values = explainer.shap_values(X_test_np)

# SHAP Summary Plot (Overall Importance - Bar Plot)
# This plot is sufficient for summarizing feature importance.
plt.figure(figsize=(12, 10))
# Using X_test_np with feature_names fixes the dimensional/Pandas error.
shap.summary_plot(shap_values, X_test_np, plot_type="bar", show=False,
                  class_names=class_names, feature_names=X_test.columns.tolist())
plt.title('SHAP Feature Importance Summary', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')
plt.show()

# ============================================================================
# SECTION 11: CONFIDENCE CALIBRATION
# ============================================================================

print("\n" + "="*80)
print("STAGE 8: CONFIDENCE CALIBRATION")
print("="*80)

# Calibrate probabilities
calibrated_model = CalibratedClassifierCV(final_model, method='sigmoid', cv=5)

# === FIX APPLIED: Convert X_train and y_train to NumPy arrays for fitting ===
calibrated_model.fit(X_train.values, y_train.values)

# Calibrated predictions
# === FIX APPLIED: Convert X_test to a NumPy array for prediction ===
y_pred_calibrated = calibrated_model.predict(X_test.values)
y_pred_proba_calibrated = calibrated_model.predict_proba(X_test.values)

calibrated_accuracy = accuracy_score(y_test, y_pred_calibrated)
print(f"\n🎯 Calibrated Model Accuracy: {calibrated_accuracy*100:.2f}%")

# Identify low-confidence predictions
max_proba = y_pred_proba_calibrated.max(axis=1)
low_confidence_mask = max_proba < 0.7
low_confidence_indices = np.where(low_confidence_mask)[0]

print(f"\n⚠️ Low Confidence Predictions (< 70%): {len(low_confidence_indices)}")
print(f"Percentage of test set: {len(low_confidence_indices)/len(y_test)*100:.2f}%")

# Show top 10 misclassifications
misclassified = y_test != y_pred_calibrated
misclassified_indices = np.where(misclassified)[0]

print(f"\n❌ Total Misclassifications: {len(misclassified_indices)}")
print(f"\nTop 10 Misclassifications:")
# NOTE: The y_test.iloc[idx] access remains valid here as y_test is a Pandas object.
for i, idx in enumerate(misclassified_indices[:10]):
    true_label = class_names[y_test.iloc[idx]]
    pred_label = class_names[y_pred_calibrated[idx]]
    confidence = max_proba[idx]
    print(f"{i+1}. True: {true_label}, Predicted: {pred_label}, Confidence: {confidence:.2%}")

# ============================================================================
# SECTION 13: SAVE MODEL & RESULTS
# ============================================================================

print("\\n" + "="*80)
print("STAGE 10: SAVING MODELS & RESULTS")
print("="*80)

# Save models
import pickle

with open('model_xgboost_exoplanet.pkl', 'wb') as f:
    pickle.dump(final_model, f)
print("✅ Saved: model_xgboost_exoplanet.pkl")

with open('model_calibrated_exoplanet.pkl', 'wb') as f:
    pickle.dump(calibrated_model, f)
print("✅ Saved: model_calibrated_exoplanet.pkl")

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("✅ Saved: scaler.pkl")

with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(le, f)
print("✅ Saved: label_encoder.pkl")

# Save predictions
results_df = pd.DataFrame({
    'true_label': [class_names[i] for i in y_test],
    'predicted_label': [class_names[i] for i in y_pred_calibrated],
    'confidence': max_proba,
    'prob_candidate': y_pred_proba_calibrated[:, 0],
    'prob_confirmed': y_pred_proba_calibrated[:, 1] if len(class_names) > 1 else 0,
    'prob_false_positive': y_pred_proba_calibrated[:, 2] if len(class_names) > 2 else 0,
})

results_df.to_csv('kepler_predictions.csv', index=False)
print("✅ Saved: kepler_predictions.csv")

# ============================================================================
# SECTION 13: SAVE MODEL & RESULTS
# ============================================================================

print("\\n" + "="*80)
print("STAGE 10: SAVING MODELS & RESULTS")
print("="*80)

# Save models
import pickle

with open('model_xgboost_exoplanet.pkl', 'wb') as f:
    pickle.dump(final_model, f)
print("✅ Saved: model_xgboost_exoplanet.pkl")

with open('model_calibrated_exoplanet.pkl', 'wb') as f:
    pickle.dump(calibrated_model, f)
print("✅ Saved: model_calibrated_exoplanet.pkl")

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("✅ Saved: scaler.pkl")

with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(le, f)
print("✅ Saved: label_encoder.pkl")

# Save predictions
results_df = pd.DataFrame({
    'true_label': [class_names[i] for i in y_test],
    'predicted_label': [class_names[i] for i in y_pred_calibrated],
    'confidence': max_proba,
    'prob_candidate': y_pred_proba_calibrated[:, 0],
    'prob_confirmed': y_pred_proba_calibrated[:, 1] if len(class_names) > 1 else 0,
    'prob_false_positive': y_pred_proba_calibrated[:, 2] if len(class_names) > 2 else 0,
})

results_df.to_csv('kepler_predictions.csv', index=False)
print("✅ Saved: kepler_predictions.csv")

# ============================================================================
# SECTION 14: PERFORMANCE REPORT
# ============================================================================

print("\\n" + "="*80)
print("FINAL PERFORMANCE REPORT")
print("="*80)

report = f"""
╔══════════════════════════════════════════════════════════════════╗
║          NASA KEPLER EXOPLANET CLASSIFICATION REPORT             ║
╚══════════════════════════════════════════════════════════════════╝

📊 DATASET STATISTICS
├─ Total Samples: {len(df_clean)}
├─ Training Samples: {len(X_train)}
├─ Testing Samples: {len(X_test)}
├─ Number of Features: {X_scaled.shape[1]}
└─ Number of Classes: {len(class_names)}

🎯 MODEL PERFORMANCE
├─ XGBoost Accuracy: {accuracy*100:.2f}%
├─ Calibrated Accuracy: {calibrated_accuracy*100:.2f}%


✅ TARGET STATUS: {'ACHIEVED ✓' if accuracy >= 0.95 else 'NOT MET ✗'}

📈 KEY METRICS
├─ Precision (Macro Avg): {classification_report(y_test, y_pred, output_dict=True)['macro avg']['precision']:.4f}
├─ Recall (Macro Avg): {classification_report(y_test, y_pred, output_dict=True)['macro avg']['recall']:.4f}
└─ F1-Score (Macro Avg): {classification_report(y_test, y_pred, output_dict=True)['macro avg']['f1-score']:.4f}

🔍 PREDICTIONS ANALYSIS
├─ Correct Predictions: {(y_test == y_pred_calibrated).sum()}
├─ Misclassifications: {len(misclassified_indices)}
├─ Low Confidence (< 70%): {len(low_confidence_indices)}
└─ High Confidence (>= 90%): {(max_proba >= 0.9).sum()}

📁 OUTPUTS GENERATED
├─ Models: model_xgboost_exoplanet.pkl, model_calibrated_exoplanet.pkl
├─ Scalers: scaler.pkl, label_encoder.pkl
├─ Predictions: kepler_predictions.csv
└─ Visualizations: 8 PNG files

🚀 NEXT STEPS
├─ Deploy model via Flask/FastAPI endpoint
├─ Integrate with ExoDiscover app backend
└─ Set up monitoring and retraining pipeline

═══════════════════════════════════════════════════════════════════
Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
═══════════════════════════════════════════════════════════════════
"""

print(report)

# Save report to file
with open('performance_report.txt', 'w') as f:
    f.write(report)
print("\\n✅ Saved: performance_report.txt")

print("\\n" + "="*80)
print("🎉 PIPELINE COMPLETE!")
print("="*80)
print("All models, visualizations, and reports have been generated.")
print("You can now deploy this model to production or integrate with your app.")